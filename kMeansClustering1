library(tm)
library(stringr)
library(lettercase)
library(tidytext)
library(syuzhet)
library(dplyr)
library(class)
library(randomForest)

train = read.csv("C:\\Users\\Iita\\Documents\\machinelearningresources\\commenttrain.csv")
test = read.csv("C:\\Users\\Iita\\Documents\\machinelearningresources\\commenttest.csv")

allcom = rbind(train[,-(3:8)], test)
allcom$comment_text = gsub("\n", " ", allcom$comment_text )


corpsify = function(x){
  x = tm_map(x, tolower)
  x = tm_map(x, removePunctuation)
  x = tm_map(x, removeNumbers)
  x = tm_map(x, removeWords, c(stopwords("english")))
  x = tm_map(x, stripWhitespace)
  x = tm_map(x, stemDocument)
  return(x)
}


# needs data in data$comment_text format
create_freq = function(data){
  corpus = Corpus(VectorSource(data))
  corpus = corpsify(corpus)
  frequencyvector = DocumentTermMatrix(corpus)
  frequencyvector = removeSparseTerms(frequencyvector, 0.995)
  freq = colSums(as.matrix(frequencyvector))
  return(freq)
}

create_top = function(x){
  top500 = names(x[tail(order(x), n=500L)])
}

create_500 = function(data){create_top(create_freq(data))} #data$comment_text!


list_subsets = list(subset(train$comment_text, train$toxic==1), subset(train$comment_text, train$severe_toxic==1),
                    subset(train$comment_text, train$obscene==1), subset(train$comment_text, train$threat==1), subset(train$comment_text, train$insult==1),
                    subset(train$comment_text, train$identity_hate==1), subset(train$comment_text, train$toxic==0))




list_tops = lapply(list_subsets, create_500)


#removing words in common
removecommon = function(topwords1, topwords2){
  topwords1 = setdiff(topwords1, intersect(topwords1,topwords2))
  return(topwords1)
}

# to remove top 500 in all words from top 500 toxic words, 
# topwords1 = top_toxic, topwords2 = top_all
cleantoplist = list()
x=1
while(x <length(list_tops)){
  cleantoplist[[x]] = removecommon(list_tops[[x]],list_tops[[length(list_tops)]])
  x = x+1
}

cleantoplist

cleanneutral = 1
y = 1
while(y < length(list_tops)){
  cleanneutral = removecommon(list_tops[length(list_tops)], list_tops[y])
  y = y+1
}

cleantoplist = c(cleantoplist, cleanneutral)

cleantoplist


#########################################################################################
# generating full set of data for proportion of given corpus' words in given comment


#list_freqs = lapply(list_subsets, create_freq)
#useless so far


frequencies_all = list()
i = 1
while(i <= length(allcom$comment_text)){
  frequencies_all[[i]] = names(create_freq(allcom$comment_text[i]))
  i = i+1
}


comparecorpus = function(topwords, commentwords){
  length(intersect(topwords, commentwords))/length(commentwords)
}


proportions = function(topwords){
  wordlistvector = 1:20
  for(i in 1:length(allcom$comment_text)){
    wordlistvector[i] = comparecorpus(topwords, frequencies_all[[i]])
  }
  return(wordlistvector)
}

#returns a vector of all comments' proportion of top toxic words, hopefully
comments_toxic = proportions(cleantoplist[[1]])

corpusproportions = list() # will end up as a list of all cleaned toplists' proportions to all comments; 7 superlists
x = 1
while(x <= length(cleantoplist)){
  corpusproportions[[x]] = proportions(cleantoplist[[x]])
  x = x+1
}



#sentiment generation, in groups because previous method was so heavy
sentiment1 = get_sentiment(allcom$comment_text[1:52000], method="afinn")
sentiment2 = get_sentiment(allcom$comment_text[52001:104000], method = "afinn")
sentiment3 = get_sentiment(allcom$comment_text[104001:156000], method = "afinn")
sentiment4 = get_sentiment(allcom$comment_text[156001:208000], method="afinn")
sentiment5 = get_sentiment(allcom$comment_text[208001:260000], method="afinn")
sentiment6 = get_sentiment(allcom$comment_text[260001:312735], method="afinn")

sentiment = c(sentiment1, sentiment2, sentiment3, sentiment4, sentiment5, sentiment6)




corpus_proportions = cbind(corpusproportions[[1]], corpusproportions[[2]],corpusproportions[[3]],
                           corpusproportions[[4]], corpusproportions[[5]], corpusproportions[[6]],
                           corpusproportions[[7]], sentiment)

corpus_proportions[is.na(corpus_proportions)==TRUE] = 0


normalize = function(x){
  return((x-min(x))/ (max(x)-min(x)))
}

corpus_proportions[,8] = normalize(corpus_proportions[,8]) #maybe try without


summary(corpus_proportions)



# this is just what needs to be done to comments to get anything useful out of them outside the corpus

cleancomments = function(cleaned){
  cleaned = removePunctuation(cleaned)
  cleaned = removeNumbers(cleaned)
  cleaned = stripWhitespace(cleaned)
  cleaned = gsub("\n", "",cleaned)
  return(cleaned)
}



caps = allcom$comment_text
caps = cleancomments(caps)

caps = strsplit(caps, "")

capssum = lapply(caps, is_upper)
capssum = lapply(capssum, sum)
capssum = unlist(capssum)

commentletters = function(x){ 
  str_length(gsub(" ", "", x))
}

#needs unlist(lapply) to get vectors instead of whatever [[1]] is

commentlengths = unlist(lapply(allcom$comment_text, commentletters))
capsprop = capssum/commentlengths

corpus_proportions = cbind(1:312735,corpus_proportions, capsprop)

colnames(corpus_proportions) = c("id","toxic","severe_toxic","obscene","threat","insult","identity_hate","neutral","sentiment",
                                 "capsprop")


clusters = kmeans(corpus_proportions, 10)
clustersf = as.factor(clusters$cluster)
clusters$centers

#function to get euclidian distances of all comments to centers of clusters
get_distances = function(y){
  distances_1 = list()
  for(x in 1:nrow(corpus_proportions)){
    distances_1[[x]] = dist(rbind(clusters$centers[y,],corpus_proportions[x,2:10]))
  }
  distances_1 = unlist(distances_1)
  return(distances_1)
}

all_dist = list()
for(y in 1:10){
  all_dist[[y]] = get_distances(y)
}


distances = matrix(unlist(all_dist), ncol=10, byrow=F)
colnames(distances) = c("dist1", "dist2", "dist3", "dist4", "dist5", "dist6", "dist7", "dist8", "dist9", "dist10")


#matrix of distances with ID to make it possible to call comments from allcom$comment_text
center_m = cbind(corpus_proportions[,1], distances)

#text of comments that are less than 0.02 away from the center of the 3rd cluster (ID of distances where distance-min(distance)<0.02)
allcom$comment_text[center_m[which((center_m[,3]-min(center_m[,3]))<0.01),1]]


get_closest = function(clustern, distance){
  centroid = allcom$comment_text[center_m[which((center_m[,clustern]-min(center_m[,clustern]))<distance),1]]
  return(centroid)
}

get_center = function(clustern){
  centroid = center_m[which.min(center_m[,clustern]),]
  return(centroid)
}

get_closest(5,0.01)
get_center(5)

allcom$comment_text[get_center(5)[1]]

centers = list()

for(i in 2:ncol(center_m)){
  centers[[i-1]] = get_center(i)[2:11]
}
library("plotly")

centers_m = matrix(unlist(centers),nrow=10, byrow=T)

plot_ly(type="scatter3d", x=center_m[,2], y=center_m[,3], z=center_m[,4], mode="markers")
plot_ly(as.data.frame(corpus_proportions), x=~toxic, y=~neutral, z= ~capsprop, color=~sentiment)

allcom$comment_text[centers[[2]]]

train1 = corpus_proportions[1:159571,]
clusters_1 = kmeans(train1, 10)$cluster
clustersf_1 = as.factor(clusters_1)
par(mfrow=c(2,2))

plot(train1[,1], train1[,7], col=clustersf_1, pch=train[,3], xlab="% of toxic words", ylab="% of neutral words")
plot(train1[,1], train1[,8], col=clustersf_1, pch=train[,3], xlab="% of toxic words", ylab="sentiment value")
plot(train1[,1], train1[,9], col=clustersf_1, pch=train[,3], xlab="% of toxic words", ylab="% of uppercase letters")
plot(train1[,1], train1[,2], col=clustersf_1, pch=train[,3], xlab="% of toxic words", ylab="% of severe toxic words")

test1 = corpus_proportions[159572:312735,]


alldata = cbind(corpus_proportions, distances)

train_1 = alldata[1:159571,]
test_1 = alldata[159571:312735,]
train_labels = train[1:159571,3:8]
colnames(train_labels) = c("Toxic", "Severe", "Obscene", "Threat", "Insult", "Identity")

train_2 = cbind(train_1,train_labels)

tree_create = function(label){
  randomForest(as.factor(label) ~ toxic+severe_toxic+obscene+threat+insult+identity_hate+neutral+sentiment+capsprop
               +dist1+dist2+dist3+dist4+dist5+dist6+dist7+dist8+dist9+dist10, data=train_2, importance=TRUE, ntree=200)
}


start_time = Sys.time()

trees = list()
#for(u in range(21:26)){
#  trees[[u]] = tree_create(train_2[,u])
#}

trees[[1]] = tree_create(train_2[,21])
trees[[2]] = tree_create(train_2[,22])
trees[[3]] = tree_create(train_2[,23])
trees[[4]] = tree_create(train_2[,24])
trees[[5]] = tree_create(train_2[,25])
trees[[6]] = tree_create(train_2[,26])



predictions = list()
for(i in 1:length(trees)){
  predictions[[i]] = predict(trees[[i]], test_1,type="prob")[,2]
}

end_time = Sys.time()

(end_time - start_time)
