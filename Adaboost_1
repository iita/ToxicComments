library(tm)
library(stringr)
library(lettercase)
library(tidytext)
library(syuzhet)
library(dplyr)
library(class)
library(randomForest)
library(adabag)
library(beepr)

train = read.csv("C:\\Users\\Iita\\Documents\\machinelearningresources\\commenttrain.csv")
test = read.csv("C:\\Users\\Iita\\Documents\\machinelearningresources\\commenttest.csv")

allcom = rbind(train[,-(3:8)], test)
allcom$comment_text = gsub("\n", " ", allcom$comment_text )

corpsify = function(x){
  x = tm_map(x, tolower)
  x = tm_map(x, removePunctuation)
  x = tm_map(x, removeNumbers)
  x = tm_map(x, removeWords, c(stopwords("english")))
  x = tm_map(x, stripWhitespace)
  x = tm_map(x, stemDocument)
  return(x)
}

create_freq = function(data){
  corpus = Corpus(VectorSource(data$comment_text))
  corpus = corpsify(corpus)
  frequencyvector = DocumentTermMatrix(corpus)
  frequencyvector = removeSparseTerms(frequencyvector, 0.98)
  freq = colSums(as.matrix(frequencyvector))
  return(freq)
}


list_subsets = list(subset(train, train$toxic==1), subset(train, train$severe_toxic==1),
                    subset(train, train$obscene==1), subset(train, train$threat==1), subset(train, train$insult==1),
                    subset(train, train$identity_hate==1), subset(train, train$toxic==0))




list_freqs = lapply(list_subsets, create_freq)
list_ord_freqs = lapply(list_freqs, order)


top500 = function(x,y){
  names(x[tail(y, n=500L)])
}

toplist = list(1,2,3,4,5,6,7)
i=1
while(i <= length(list_freqs)){
  toplist[[i]] = top500(list_freqs[[i]], list_ord_freqs[[i]])
  i = i + 1
}




#removing words in common
removecommon = function(topwords1, topwords2){
  topwords1 = setdiff(topwords1, intersect(topwords1,topwords2))
  return(topwords1)
}

# to remove top 500 in all words from top 500 toxic words, 
# topwords1 = top_toxic, topwords2 = top_all
cleantoplist = list(1,2,3,4,5,6)
x=1
while(x <length(list_freqs)){
  cleantoplist[[x]] = removecommon(toplist[[x]],toplist[[length(toplist)]])
  x = x+1
}

cleantoplist

cleanneutral = 1
y = 1
while(y < length(toplist)){
  cleanneutral = removecommon(toplist[length(toplist)], toplist[y])
  y = y+1
}

cleantoplist = c(cleantoplist, cleanneutral)

cleantoplist


#########################################################################################
# generating full set of data for proportion of given corpus' words in given comment

#creating a vector of frequencies for corpuses (corpusii, not corpsii, but whatever)
#corpsii is useless on its own, just there to help with frequencies

# creates a function to return frequencies of a single comment's corpus (just to get the names later)
# so fucking roundabout I know


#data needs to be data$comment_text
create_freq1 = function(data){
  corpus = Corpus(VectorSource(data))
  corpus = corpsify(corpus)
  frequencyvector = DocumentTermMatrix(corpus)
  frequencyvector = removeSparseTerms(frequencyvector, 0.98)
  freq = colSums(as.matrix(frequencyvector))
  return(freq)
}

create_freq1(toxic$comment_text)[1] # returns first word in toxic corpus freq
create_freq1(toxic$comment_text[1]) # returns freq of first comment in toxic set
frequencies_all = list()

i = 1
while(i <= length(allcom$comment_text)){
  frequencies_all[[i]] = names(create_freq1(allcom$comment_text[i]))
  i = i+1
}


comparecorpus = function(topwords, commentwords){
  length(intersect(topwords, commentwords))/length(commentwords)
}


proportions = function(topwords){
  wordlistvector = 1:20
  for(i in 1:length(allcom$comment_text)){
    wordlistvector[i] = comparecorpus(topwords, frequencies_all[[i]])
  }
  return(wordlistvector)
}

#returns a vector of all comments' proportion of top toxic words, hopefully
comments_toxic = proportions(cleantoplist[[1]])

corpusproportions = list() # will end up as a list of all cleaned toplists' proportions to all comments; 7 superlists
x = 1
while(x <= length(cleantoplist)){
  corpusproportions[[x]] = proportions(cleantoplist[[x]])
  x = x+1
}



#sentiment generation, in groups because previous method was so heavy
sentiment1 = get_sentiment(allcom$comment_text[1:52000], method="afinn")
sentiment2 = get_sentiment(allcom$comment_text[52001:104000], method = "afinn")
sentiment3 = get_sentiment(allcom$comment_text[104001:156000], method = "afinn")
sentiment4 = get_sentiment(allcom$comment_text[156001:208000], method="afinn")
sentiment5 = get_sentiment(allcom$comment_text[208001:260000], method="afinn")
sentiment6 = get_sentiment(allcom$comment_text[260001:312735], method="afinn")

sentiment = c(sentiment1, sentiment2, sentiment3, sentiment4, sentiment5, sentiment6)

sentiment7 = get_sentiment(allcom$comment_text[1:52000], method="nrc")
sentiment8 = get_sentiment(allcom$comment_text[52001:104000], method = "nrc")
sentiment9 = get_sentiment(allcom$comment_text[104001:156000], method = "nrc")
sentiment10 = get_sentiment(allcom$comment_text[156001:208000], method="nrc")
sentiment11 = get_sentiment(allcom$comment_text[208001:260000], method="nrc")
sentiment12 = get_sentiment(allcom$comment_text[260001:312735], method="nrc")

sentiments2 = c(sentiment7, sentiment8, sentiment9, sentiment10, sentiment11, sentiment12)




sentiments_check = cbind(corpusproportions[[1]], corpusproportions[[2]],corpusproportions[[3]],
                         corpusproportions[[4]], corpusproportions[[5]], corpusproportions[[6]],
                         corpusproportions[[7]], sentiment, sentiments2)

sentiments_check[is.na(sentiments_check)==TRUE] = 0


normalize = function(x){
  return((x-min(x))/ (max(x)-min(x)))
}

sentiments_check[,8] = normalize(sentiments_check[,8])
sentiments_check[,9] = normalize(sentiments_check[,9])

summary(sentiments_check)



punctuation1 = allcom$comment_text
punctuation1 = gsub("\n", "", punctuation1)

sum_punctuation = function(x, punctuation){
  sum(gregexpr(punctuation,x)[[1]]>0)
}

punc_options = list("[!?,.]","[!]","[?]","[,]","[.]" )

puncprops = list()
y = 1
while(y<=length(punc_options)){
  puncprops[[y]] = unlist(lapply(punctuation1,sum_punctuation,punctuation = punc_options[[y]]))
  y = y+1
}
puncprops_df = cbind(puncprops[[1]], puncprops[[2]], puncprops[[3]], puncprops[[4]], puncprops[[5]])
colnames(puncprops_df) = c("allpunc", "exclam", "quest", "comma", "period")

#proportion of exclamation marks to all punctuation
expro = puncprops_df[,2] / puncprops_df[,1]
expro[is.nan(expro)==TRUE] = 0

puncprops_df = cbind(puncprops_df, expro)


#remove /n, return comment character count
charactercount = function(x){
  x = gsub("\n", " ", x)
  nchar(x)
}

characters = unlist(lapply(punctuation1, charactercount))

#proportion of punctuation to length of comment
props = function(x){
  return(x/characters)
}

puncprops2 = list()
y=1
while(y<=length(punc_options)){
  puncprops2[[y]] = props(puncprops_df[,y])
  y=y+1
}

puncprops_df = cbind(puncprops_df, puncprops2[[1]], puncprops2[[2]], puncprops2[[3]], puncprops2[[4]], puncprops2[[5]])

colnames(puncprops_df) = c("allpunc", "exclam", "quest", "comma", "period", "expro", "allpuncprop", "exclprop", "qprop",
                           "commaprop", "periodprop")

# this is just what needs to be done to comments to get anything useful out of them outside the corpus

cleancomments = function(cleaned){
  cleaned = removePunctuation(cleaned)
  cleaned = removeNumbers(cleaned)
  cleaned = stripWhitespace(cleaned)
  cleaned = gsub("\n", "",cleaned)
  return(cleaned)
}



caps = allcom$comment_text
caps = cleancomments(caps)

caps = strsplit(caps, "")

capssum = lapply(caps, is_upper)
capssum = lapply(capssum, sum)
capssum = unlist(capssum)

commentletters = function(x){ 
  str_length(gsub(" ", "", x))
}

#needs unlist(lapply) to get vectors instead of whatever [[1]] is

commentlengths = unlist(lapply(punctuation1, commentletters))
capsprop = capssum/commentlengths

puncprops_df = cbind(puncprops_df, capsprop)



normalize = function(x){
  return((x-min(x))/ (max(x)-min(x)))
}


secondary_nor = apply(puncprops_df, 2, normalize)

sentiments_check = cbind(sentiments_check, secondary_nor)

colnames(sentiments_check) = c("toxic","severe_toxic","obscene","threat","insult","identity_hate",
                               "neutral","sentiment",
                               "sentiments2", "allpunc", "exclam", "quest", "comma", "period", "expro", "allpuncprop", "exclprop",
                               "qprop","commaprop", "periodprop", "capsprop")


train1 = sentiments_check[1:159571,]
test1 = sentiments_check[159572:312735,]

train_labels1 = train[1:159571,3:8]

train_labels_toxic = train_labels1[,1]
train_labels_severe_toxic = train_labels1[,2]
train_labels_obscene = train_labels1[,3]
train_labels_threat = train_labels1[,4]
train_labels_insult = train_labels1[,5]
train_labels_identity_hate = train_labels1[,6]

train2 = cbind(train_labels1,train1)
colnames(train2) = c("Toxic", "Severe_toxic", "Obscene", "Threat", "Insult", "Identity_hate",
                     "toxic","severe_toxic","obscene","threat","insult","identity_hate",
                     "neutral","sentiment",
                     "sentiments2","allpunc", "exclam", "quest", "comma", "period", "expro", "allpuncprop", "exclprop",
                     "qprop","commaprop", "periodprop", "capsprop")

train3 = train2
train3[,1] = as.factor(train3[,1])
train3[,2] = as.factor(train3[,2])
train3[,3] = as.factor(train3[,3])
train3[,4] = as.factor(train3[,4])
train3[,5] = as.factor(train3[,5])
train3[,6] = as.factor(train3[,6])

boost1 = boosting(Toxic ~ toxic + severe_toxic + obscene + threat + insult + identity_hate + neutral +
                       sentiment + sentiments2 + allpuncprop + exclprop + qprop + expro +
                       periodprop + capsprop, data=train3, boos=TRUE, mfinal=20, coeflearn = "Breiman", rpart.control(minsplit = 0))

rockets = function(label){
  boost = boosting(label ~ toxic + severe_toxic + obscene + threat + insult + identity_hate + neutral +
                     sentiment + sentiments2 + allpuncprop + exclprop + qprop + expro +
                     periodprop + capsprop, data=train3, boos=TRUE, mfinal=20, coeflearn = "Breiman", rpart.control(minsplit = 0))
  return(boost)
}

boost1$trees
boost1$weights
boost1$importance
plot(boost1$trees[[1]])

fancyRpartPlot(boost1$trees[[5]])

test2 = as.data.frame(test1)
pred_boost1 = predict(boost1,test2, type="prob")


boost2 = boosting(Severe_toxic ~ toxic + severe_toxic + obscene + threat + insult + identity_hate + neutral +
                    sentiment + sentiments2 + allpuncprop + exclprop + qprop + expro +
                    periodprop + capsprop, data=train3, boos=TRUE, mfinal=20, coeflearn = "Breiman", rpart.control(minsplit = 0))

boost3 = boosting(Obscene ~ toxic + severe_toxic + obscene + threat + insult + identity_hate + neutral +
                    sentiment + sentiments2 + allpuncprop + exclprop + qprop + expro +
                    periodprop + capsprop, data=train3, boos=TRUE, mfinal=20, coeflearn = "Breiman", rpart.control(minsplit = 0))


boost4 = boosting(Threat ~ toxic + severe_toxic + obscene + threat + insult + identity_hate + neutral +
                    sentiment + sentiments2 + allpuncprop + exclprop + qprop + expro +
                    periodprop + capsprop, data=train3, boos=TRUE, mfinal=20, coeflearn = "Breiman", rpart.control(minsplit = 0))


boost5 = boosting(Insult ~ toxic + severe_toxic + obscene + threat + insult + identity_hate + neutral +
                    sentiment + sentiments2 + allpuncprop + exclprop + qprop + expro +
                    periodprop + capsprop, data=train3, boos=TRUE, mfinal=20, coeflearn = "Breiman", rpart.control(minsplit = 0))


boost6 = boosting(Identity_hate ~ toxic + severe_toxic + obscene + threat + insult + identity_hate + neutral +
                    sentiment + sentiments2 + allpuncprop + exclprop + qprop + expro +
                    periodprop + capsprop, data=train3, boos=TRUE, mfinal=20, coeflearn = "Breiman", rpart.control(minsplit = 0))


boost_toxic = boost1
boost_severe_toxic = boost2
boost_obscene = boost3
boost_threat = boost4
boost_insult = boost5
boost_identity_hate = boost6

pred_prob = function(x){
  predict(x, test1, type="prob")$prob[,2]
}

boosts = list(boost_toxic, boost_severe_toxic, boost_obscene, boost_threat, boost_insult, boost_identity_hate)

predictions = list()
for(i in 1:length(boosts)){
  predictions[[i]] = pred_prob(boosts[[i]])
}

solution = data.frame(id=test[,1], toxic=predictions[[1]], severe_toxic=predictions[[2]],
                      obscene=predictions[[3]], threat=predictions[[4]], insult=predictions[[5]],
                      identity_hate=predictions[[6]])

write.table(solution, "adaboost_1.csv", row.names = FALSE, sep=",", eol="\r", quote = FALSE)
beep(1)
